import pytest
from gliner import GLiNER
from pathlib import Path

# Load the model (downloads automatically on first use)
know_model = GLiNER.from_pretrained("nvidia/gliner-pii")
file_path = (Path(__file__).parent / ".." / "resources" / "raw_text_confluence.txt").resolve()
text = file_path.read_text(encoding="utf-8")

# Natural language labels - deduplicated
labels = [
    "iban",
    "ssn"
]


print(f"\nUsing {len(labels)} labels (natural language only, no uppercase)\n")
first_text = text[:500]
second_text = text[500:1000]
third_text = text[1000:1500]
fourth_text = text[1500:2000]
#fifth_text = text[2000:2500]

entities = know_model.predict_entities(first_text, labels, threshold=0.2)
print("\n")
for entity in entities:
    print(f"{entity['text']} => {entity['label']} (confidence: {entity['score']:.2f})")
print("\n\n")

entities = know_model.predict_entities(second_text, labels, threshold=0.2)
print("\n")
for entity in entities:
    print(f"{entity['text']} => {entity['label']} (confidence: {entity['score']:.2f})")
print("\n\n")

entities = know_model.predict_entities(third_text, labels, threshold=0.2,multi_label=True)
print("\n")
for entity in entities:
    print(f"{entity['text']} => {entity['label']} (confidence: {entity['score']:.2f})")
print("\n\n")

entities = know_model.predict_entities(fourth_text, labels, threshold=0.2,multi_label=True)
print("\n")
for entity in entities:
    print(f"{entity['text']} => {entity['label']} (confidence: {entity['score']:.2f})")
print("\n\n")
# entities = know_model.predict_entities(fifth_text, labels, threshold=0.2)
# print("\n")
# for entity in entities:
#     print(f"{entity['text']} => {entity['label']} (confidence: {entity['score']:.2f})")
# print("\n\n")
#entities = urchade_model.predict_entities(text, labels, threshold=0.2,multi_label=True)
#for entity in entities:
#    print(f"{entity['text']} => {entity['label']} (confidence: {entity['score']:.2f})")

# GLiNER isn't limited to PII - you can detect any entities
#text = "The MacBook Pro with M2 chip costs $1,999 at the Apple Store in Manhattan."
#custom_labels = ["product", "processor", "price", "store", "location"]
#print("\n\n")
#entities = know_model.predict_entities(text, custom_labels, threshold=0.3)
#for entity in entities:
#    print(f"{entity['text']} => {entity['label']} (confidence: {entity['score']:.2f})")

# import transformers
# import torch
# import time
# model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
#
# pipeline = transformers.pipeline(
#     "text-generation",
#     model=model_id,
#     model_kwargs={"torch_dtype": torch.bfloat16},
#     device_map="auto",
# )
#
# messages = [
#     {"role": "system", "content": "You are an expert at findings PII (Personally identifiable Information) in text. output must be a key value object with PII_CATEGORY: [PII]"},
#     {"role": "user", "content": """Find the following  PII  [USERNAME,FIRST_NAME,LAST_NAME,EMAIL,API_KEY,DB_CONN_STRING, AVS_NUMBER, IBAN] in the following text:\n Proc√©dure de d√©ploiement ‚Äì Environnement pr√©prod
#
#      Objectif
#
#          Ce document d√©crit le processus de d√©ploiement de l'application "DataBridge" sur l'environnement pr√©production (PREPROD2-VD). Il est √† usage interne uniquement.
#
#     Informations d'acc√®s Confluence et outils
#
#      Utilisateur syst√®me : j.doe (Responsable DevOps)
# Email r√©f√©rent : jean.dupont@example.com
#
# Compte de service API : svc-deploy@databridge.local
#
# Acc√®s VPN n√©cessaire (voir section s√©curit√©)
#
# Acc√®s Confluence : https://intra.vd.ch/wiki
#
# Variables d'environnement (√† injecter dans le docker-compose.override.yml)
# wide760DB_USER: admin_vd
# DB_PASS: P@ssw0rd!2024
# CONFLUENCE_TOKEN: ATATT3xFfGF0y7EXAMPLE
# POSTGRES_URL: jdbc:postgresql://db-internal.vd.ch:5432/databridge
#
# üîí Note : ces variables sont inject√©es dynamiquement par Infisical en environnement s√©curis√©. Ne pas versionner ce fichier.
#
# √âtapes de d√©ploiement
#
# Se connecter au bastion via SSH :
#
# wide760ssh -i ~/.ssh/id_ed25519 j.doe@bastion.vd.ch
#
# R√©cup√©rer les derniers artefacts sur Nexus
#
# Lancer le script de mise √† jour :
#
# wide760./deploy.sh --env preprod2
#
# Cl√©s et identifiants (ne pas diffuser)
#
# Cl√© API OpenAI pour module r√©sum√© : sk-test-Y9yJW2TfkjYxEXAMPLE
#
# AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
#
# AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
#
# Journal de test (extrait du 2024-11-04)
#
# Le service "PDF Extractor" remonte une erreur 403 lors du traitement des documents confidentiels. Possible cause : token JWT expir√© ou r√¥le manquant.
#
# Ancien token utilis√© : eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0...
#
# Suivi utilisateur
#
# Jean Dupont (DPO) a valid√© le processus. Num√©ro AVS r√©f√©rent : 756.9217.0769.85 RIB √† usage interne : CH93 0076 2011 6238 5295 7
#
# Annexe ‚Äì Extrait page Confluence
#
# La base de donn√©es contient les identifiants initiaux suivants (DO NOT USE in prod)
#
# Utilisateur
#
# Mot de passe
#
# root
#
# changeme123!
#
# support
#
# VdSupport2023*"""},
# ]
# start_time = time.time()
#
# outputs = pipeline(
#     messages,
#     max_new_tokens=256,
# )
# elapsed_time = time.time() - start_time
# print("Elapsed time:", elapsed_time, "seconds")
# print(outputs[0]["generated_text"][-1])